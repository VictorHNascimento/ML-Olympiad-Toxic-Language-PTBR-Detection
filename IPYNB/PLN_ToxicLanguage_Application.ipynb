{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Funções"
      ],
      "metadata": {
        "id": "tp0K6MRz5rKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji\n",
        "!pip install unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8As8mgqN2jFw",
        "outputId": "b2a0d05e-7eda-4fb3-c2a7-351abcd74837"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (2.12.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from emoji) (4.11.0)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.10/dist-packages (1.3.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import joblib\n",
        "import pandas as pd\n",
        "from scipy.sparse import hstack\n",
        "import pickle\n",
        "import sys\n",
        "import emoji\n",
        "from unidecode import unidecode"
      ],
      "metadata": {
        "id": "P2QKmJBeVxLn"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jo7BCMlLU0jK",
        "outputId": "872e3dd7-4de6-4654-e412-58d472168740"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-31 00:17:22--  https://github.com/VictorHNascimento/ML-Olympiad-Toxic-Language-PTBR-Detection/raw/main/toxic_words.pkl\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/VictorHNascimento/ML-Olympiad-Toxic-Language-PTBR-Detection/main/toxic_words.pkl [following]\n",
            "--2024-05-31 00:17:22--  https://raw.githubusercontent.com/VictorHNascimento/ML-Olympiad-Toxic-Language-PTBR-Detection/main/toxic_words.pkl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 212765 (208K) [application/octet-stream]\n",
            "Saving to: ‘toxic_words.pkl’\n",
            "\n",
            "toxic_words.pkl     100%[===================>] 207.78K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-05-31 00:17:22 (5.96 MB/s) - ‘toxic_words.pkl’ saved [212765/212765]\n",
            "\n",
            "--2024-05-31 00:17:22--  https://github.com/VictorHNascimento/ML-Olympiad-Toxic-Language-PTBR-Detection/raw/main/toxic_words.pkl\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/VictorHNascimento/ML-Olympiad-Toxic-Language-PTBR-Detection/main/toxic_words.pkl [following]\n",
            "--2024-05-31 00:17:23--  https://raw.githubusercontent.com/VictorHNascimento/ML-Olympiad-Toxic-Language-PTBR-Detection/main/toxic_words.pkl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 212765 (208K) [application/octet-stream]\n",
            "Saving to: ‘non_toxic_words.pkl’\n",
            "\n",
            "non_toxic_words.pkl 100%[===================>] 207.78K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-05-31 00:17:23 (6.03 MB/s) - ‘non_toxic_words.pkl’ saved [212765/212765]\n",
            "\n",
            "--2024-05-31 00:17:23--  https://github.com/VictorHNascimento/ML-Olympiad-Toxic-Language-PTBR-Detection/raw/main/vectorizer.pkl\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/VictorHNascimento/ML-Olympiad-Toxic-Language-PTBR-Detection/main/vectorizer.pkl [following]\n",
            "--2024-05-31 00:17:23--  https://raw.githubusercontent.com/VictorHNascimento/ML-Olympiad-Toxic-Language-PTBR-Detection/main/vectorizer.pkl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 663246 (648K) [application/octet-stream]\n",
            "Saving to: ‘vectorizer.pkl’\n",
            "\n",
            "vectorizer.pkl      100%[===================>] 647.70K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-05-31 00:17:23 (11.7 MB/s) - ‘vectorizer.pkl’ saved [663246/663246]\n",
            "\n",
            "--2024-05-31 00:17:23--  https://github.com/VictorHNascimento/ML-Olympiad-Toxic-Language-PTBR-Detection/raw/main/identifica_toxicos.pkl\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/VictorHNascimento/ML-Olympiad-Toxic-Language-PTBR-Detection/main/identifica_toxicos.pkl [following]\n",
            "--2024-05-31 00:17:24--  https://raw.githubusercontent.com/VictorHNascimento/ML-Olympiad-Toxic-Language-PTBR-Detection/main/identifica_toxicos.pkl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 364175 (356K) [application/octet-stream]\n",
            "Saving to: ‘model.pkl’\n",
            "\n",
            "model.pkl           100%[===================>] 355.64K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-05-31 00:17:24 (7.90 MB/s) - ‘model.pkl’ saved [364175/364175]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def limpaTexto(text):\n",
        "    # Remover emojis\n",
        "    text = emoji.replace_emoji(text, replace='')  # Remove emojis\n",
        "\n",
        "    # Remover menções de usuários\n",
        "    text = re.sub(r'@\\w+', ' ', text)\n",
        "\n",
        "    # Remover links\n",
        "    text = re.sub(r'https?://\\S+', ' ', text)\n",
        "\n",
        "    # Remover espaços extras\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Remover palavras específicas\n",
        "    text = re.sub(r'\\b(rt|user|https)\\b', ' ', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Remover acentuação\n",
        "    text = unidecode(text)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "def preprocessaTexto(text):\n",
        "    text = limpaTexto(text)\n",
        "    return text\n",
        "\n",
        "# Carregar listas de palavras tóxicas e não tóxicas usando pickle\n",
        "url_toxic = 'https://github.com/VictorHNascimento/ML-Olympiad-Toxic-Language-PTBR-Detection/raw/main/toxic_words.pkl'\n",
        "!wget {url_toxic} -O toxic_words.pkl\n",
        "\n",
        "url_nt = 'https://github.com/VictorHNascimento/ML-Olympiad-Toxic-Language-PTBR-Detection/raw/main/non_toxic_words.pkl'\n",
        "!wget {url_toxic} -O non_toxic_words.pkl\n",
        "\n",
        "with open('toxic_words.pkl', 'rb') as file:\n",
        "    toxic_words = pickle.load(file)\n",
        "\n",
        "with open('non_toxic_words.pkl', 'rb') as file:\n",
        "    non_toxic_words= pickle.load(file)\n",
        "\n",
        "# Funções de extração de features\n",
        "def contaToxicos(text, toxic_words):\n",
        "  count = 0\n",
        "  for p in text.split():\n",
        "    if p in toxic_words:\n",
        "      count += 1\n",
        "  return count\n",
        "\n",
        "def contaNaotoxicos(text, nontoxic_words):\n",
        "  count = 0\n",
        "  for p in text.split():\n",
        "    if p in nontoxic_words:\n",
        "      count += 1\n",
        "  return count\n",
        "\n",
        "def contaNeutros(text, toxic_words, nontoxic_words):\n",
        "  words = text.split()\n",
        "  count = 0\n",
        "  for p in text.split():\n",
        "    if p in nontoxic_words and p in toxic_words:\n",
        "      count += 1\n",
        "  return count\n",
        "\n",
        "def contemToxicos(text, toxic_words):\n",
        "  for p in text.split():\n",
        "    if p in toxic_words:\n",
        "      return 1\n",
        "  return 0\n",
        "\n",
        "def contemNaotoxicos(text, nontoxic_words):\n",
        "  for p in text.split():\n",
        "    if p in nontoxic_words:\n",
        "      return 1\n",
        "  return 0\n",
        "\n",
        "def contemNeutros(text, toxic_words, nontoxic_words):\n",
        "  for p in text.split():\n",
        "     if p in nontoxic_words and p in toxic_words:\n",
        "      return 1\n",
        "  return 0\n",
        "\n",
        "\n",
        "# Carregar vetorizador e modelo\n",
        "url_veto = 'https://github.com/VictorHNascimento/ML-Olympiad-Toxic-Language-PTBR-Detection/raw/main/vectorizer.pkl'\n",
        "!wget {url_veto} -O vectorizer.pkl\n",
        "vetorizador = joblib.load('vectorizer.pkl')\n",
        "\n",
        "url_model = 'https://github.com/VictorHNascimento/ML-Olympiad-Toxic-Language-PTBR-Detection/raw/main/identifica_toxicos.pkl'\n",
        "!wget {url_model} -O model.pkl\n",
        "modelo = joblib.load('model.pkl')\n",
        "\n",
        "# Função principal para prever a toxicidade de um tweet\n",
        "def predizToxico(tweet):\n",
        "    # Pré-processar o texto\n",
        "    tweet = preprocessaTexto(tweet)\n",
        "\n",
        "    # Criar features\n",
        "    features = {\n",
        "        'text': tweet,\n",
        "        'toxic_words': contemToxicos(tweet, toxic_words),\n",
        "        'cont_toxic_words': contaToxicos(tweet, toxic_words),\n",
        "        'non_toxic_words': contemNaotoxicos(tweet, non_toxic_words),\n",
        "        'cont_non_toxic_words': contaNaotoxicos(tweet, non_toxic_words),\n",
        "        'neutral_words': contemNeutros(tweet, toxic_words, non_toxic_words),\n",
        "        'cont_neutral_words': contaNeutros(tweet, toxic_words, non_toxic_words),\n",
        "        'char_cont': len(tweet),\n",
        "        'words_cont': len(tweet.split())\n",
        "    }\n",
        "\n",
        "    # Transformar texto usando TF-IDF\n",
        "    X_t = vetorizador.transform([tweet])\n",
        "\n",
        "    # Criar DataFrame com outras features\n",
        "    X_others = pd.DataFrame([features]).drop(columns=['text'])\n",
        "\n",
        "    # Combinar as features\n",
        "    X = hstack([X_t, X_others])\n",
        "\n",
        "    # Fazer a predição\n",
        "    prediction = modelo.predict(X)\n",
        "\n",
        "    return prediction[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Aplicação"
      ],
      "metadata": {
        "id": "pDaGKkV35uU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função principal para entrada do usuário\n",
        "x = 1\n",
        "while x == 1:\n",
        "  if __name__ == \"__main__\":\n",
        "      tweet = input(\"Digite o tweet: \")\n",
        "      prediction = predizToxico(tweet)\n",
        "      if prediction == 1:\n",
        "          print(\"\\033[1;31mO tweet é tóxico\\033[0m\")\n",
        "      else:\n",
        "          print(\"\\033[1;32mO tweet é não tóxico\\033[0m\")\n",
        "\n",
        "      print()\n",
        "      x = int(\n",
        "          input('Digite 1 se deseja fazer uma nova predição: ')\n",
        "      )\n",
        "      print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytdTuXRy338A",
        "outputId": "ea7a4f41-b507-43e5-94e2-ad5317128ac0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Digite o tweet: Vai se foder @user, caralho, se mata\n",
            "\u001b[1;31mO tweet é tóxico\u001b[0m\n",
            "\n",
            "Digite 1 se deseja fazer uma nova predição: 1\n",
            "\n",
            "Digite o tweet: Acabei de receber um: 'assistia teus vídeos desde quando eu era criança' na academia de um cara com 3x mais braço que eu, fiquei triste\n",
            "\u001b[1;32mO tweet é não tóxico\u001b[0m\n",
            "\n",
            "Digite 1 se deseja fazer uma nova predição: 0\n",
            "\n"
          ]
        }
      ]
    }
  ]
}